# Social Distancing in Shopping malls Using YOLO and MonoDepth Estimation# Import librariesfrom mylib import config, threadfrom mylib.detection import detect_peoplefrom imutils.video import VideoStream, FPSfrom scipy.spatial import distance as distimport numpy as npimport argparse, imutils, cv2, os, time, schedule, glob, matplotlib# Keras / TensorFlowos.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'from keras.models import load_modelfrom mylib.layers import BilinearUpSampling2Dfrom mylib.utils import predict, load_images, display_imagesfrom matplotlib import pyplot as plt#----------------------------Parse req. arguments------------------------------#ap = argparse.ArgumentParser(description="Distance Estimation")# input (write the directory path in default)ap.add_argument("-i", "--input", type=str, default="470_image.png",	help="input file name, image or video")# outputap.add_argument("-o", "--output", type=str, default="",	help="path to (optional) output video file")# display (True / False)ap.add_argument("-d", "--display", type=int, default=True,	help="whether or not output frame should be displayed")# import nyu,h5 model for monodepth estimationap.add_argument("-m","--model", type=str, default="nyu.h5",  	help="Trained Keras model file.")args = vars(ap.parse_args())#------------------------------------------------------------------------------##----------------Setting up YOLO labels, weights and model---------------------## load the COCO class labels our YOLO model was trained on# if you saved COCO labels in other folder than YOLO, write your own path# instead of config.MODEL_PATHlabelsPath = os.path.sep.join([config.MODEL_PATH, "coco.names"])LABELS = open(labelsPath).read().strip().split("\n")# derive the paths to the YOLO weights and model configurationweightsPath = os.path.sep.join([config.MODEL_PATH, "yolov3.weights"])configPath = os.path.sep.join([config.MODEL_PATH, "yolov3.cfg"])# load our YOLO object detector trained on COCO dataset (80 classes)net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)#------------------------------------------------------------------------------##------------------Setting up MonoDepth weights and model----------------------## Custom object needed for inference and trainingcustom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}# Load model into GPU / CPUmodel = load_model(args["model"], custom_objects=custom_objects, compile=False)print('\nModel loaded ({0}).'.format(args["model"]))#------------------------------------------------------------------------------##----------------------------Setting up GPU------------------------------------## check if we are going to use GPU# If you dont want to use GPU assign false in configif config.USE_GPU:	# set CUDA as the preferable backend and target	print("")	print("[INFO] Looking for GPU")	net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)	net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)#------------------------------------------------------------------------------##---------------------Preparing Input to YOLO model----------------------------## grabe the refrence to the video from pars argument# if it did not work search for the URL to online camera in config# else show a warning that input data is emptytry:	print("[INFO] Starting the video..")	vs = cv2.VideoCapture(args["input"])	#frame= load_images( glob.glob(args["input"]) )	if config.Thread:			cap = thread.ThreadingClass(args["input"])except:	print('no video refrence for the video input...')	print("[INFO] Starting the live stream...")	vs = cv2.VideoCapture(config.url)	if config.Thread:			cap = thread.ThreadingClass(config.url)	time.sleep(2.0)#else:	#print('WARNING: request cannot be processed. Please enter a video refrence or camera URL for input')#------------------------------------------------------------------------------##----------------------------Processing frames---------------------------------## determine only the *output* layer names that we need from YOLO # this will be used for detecting desired object in frameln = net.getLayerNames()ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]writer = None# start the FPS counter (Frame Rate : images speed are shown)fps = FPS().start()# loop over the frames from the video streamwhile True:	# read the next frame from the file	if config.Thread:		frame = cap.read()	else:		(grabbed, frame) = vs.read()		# if the frame was not grabbed, then we have reached the end of the stream		if not grabbed:			break	#---------------------------------------------------------------------------#	# resize the frame and then detect people (and only people) in it	frame = imutils.resize(frame, width=700)	results = detect_people(frame, net, ln, personIdx=LABELS.index("person"))	# The results gives us people	#---------------------------------------------------------------------------#	# loop over the results	for (i, (prob, bbox, centroid)) in enumerate(results):		# extract the bounding box and centroid coordinates, then		# initialize the color of the annotation		(startX, startY, endX, endY) = bbox		(cX, cY) = centroid		color = (0, 255, 0)		# draw (1) a bounding box around the person and (2) the		# centroid coordinates of the person,		cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)		cv2.circle(frame, (cX, cY), 5, color, 2)		# Compute results of Monodepth Model		print('\nLoaded ({0}) images of size {1}.'.format(frame.shape[0], frame.shape[1:]))				# Resize frames to be compatible with MONOdepth network		frame.resize(480, 640, 3)		outputs = predict(model, frame)	#---------------------------------------------------------------------------#	# check to see if the output frame should be displayed to our screen	if args["display"] == True:		# show the output frame		cv2.imshow("Real-Time Monitoring/Analysis Window", frame)		key = cv2.waitKey(1) & 0xFF		plt.savefig('test.png')	#---------------------------------------------------------------------------#	# if the `q` key was pressed, break from the loop	if key == ord("q"):		break    # update the FPS counter	fps.update()	#---------------------------------------------------------------------------#	# if an output video file path has been supplied and the video	# writer has not been initialized, do so now	if args["output"] != "" and writer is None:		# initialize our video writer		fourcc = cv2.VideoWriter_fourcc(*"MJPG")		writer = cv2.VideoWriter(args["output"], fourcc, 25,			(frame.shape[1], frame.shape[0]), True)	# if the video writer is not None, write the frame to the output video file	if writer is not None:		writer.write(frame)#-------------------------------------------------------------------------------##-----------------------------End the Process-----------------------------------## stop the timer and display FPS informationfps.stop()print("===========================")print("[INFO] Elasped time: {:.2f}".format(fps.elapsed()))print("[INFO] Approx. FPS: {:.2f}".format(fps.fps()))# close any open windowscv2.destroyAllWindows()